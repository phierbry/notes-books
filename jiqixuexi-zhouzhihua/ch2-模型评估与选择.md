# ch2 模型评估与选择

从测试集上的性能度量指标到泛化能力的度量。

## 性能度量

得到的模型需要一些评估好坏的指标。我们先在测试集上评估而不考虑泛化能力。  
这里讨论二分类任务。
- 错误率与精度：错误率=分类出错的/测试集规模，精度=1-错误率    
- 查准率（precision）查重率（recall和F1：首先明确混淆矩阵的概念（TP,FP,TN,FN）。  
	- 查准率：选出来的有多少是真的好 $P = TP/(TP+FP)$  
	- 查全率：好的有多少被选出来 $R=TP/(TP+FN)$
	- F1：为什么定义F1，我们希望P和R都好，二者之间需要tradeoff。F1是P,R调和平均。
	- $F_\beta$：有时我们P和R不是同样重要的，于是加权调和平均。   
	- 多个测试集上的多个混淆矩阵：macro-, micro-
- ROC（receiver operating characteristic）与AUC（area under curve）  
	- 使用真正例作为纵轴假正例作为横轴，则x=1最佳，y=x为随机猜测
	- TPR=TP/(TP+FN), FPR=FP/(FP+TN)
	- 离散点的作图，计算代价（每一个排反的正反例+1，并列的+0.5），等价于（1-AUC）
- 代价敏感错误率 cost-sensitive   
	- FN 与 FP的代价不同，除了混淆矩阵以外还有代价矩阵  


## 假设检验  hypothesis test
性能度量仅仅如上节在测试集上的计算是不足的，真正需要度量的是泛化能力。  
我们通过假设检验的方法来从测试集上的能力推断泛化能力。 假设检验做这样的事：在测试集上观测到算法A比算法B好（或者是某个水平），那么在泛化能力上，A是否比B好（是否也有相当水平），这个结论把握有多大。这里介绍两个假设检验，和几种机器学习的性能比较方法，全部以错误率$\epsilon$作为度量。  
#### 假设检验原理
**二项检验 binomial test** 单边的，只有一个测试集，一个错误率。
我们可以计算在m个样本的测试集上，泛化错误率为$\epsilon$的算法表现错误率为$\hat\epsilon$的概率$P(\epsilon, \hat\epsilon)$。
我们作出假设“$\epsilon \leq \epsilon_0$”，设置显著度$\alpha$和置信度(confidence)$1-\alpha$。然后计算发生糟糕情况的临界值$\epsilon$，把临界值和假设上限做比较，如果小于，则以置信度$1-\alpha$接受检验。这里的糟糕情况就是差劲表现的概率超过了$\alpha$，而临界值是置信度可以容忍的最大$\epsilon$。  
**t-检验 t-test** 双边的，支持多个测试集，多个错误率。  
将算法放到k个测试集上得到k个错误率，求得均值$\mu$(1/k)和方差$\sigma^2$(1/k-1)。把这k个错误率看作泛化错误率的独立采样，于是一个关于$k,\sigma,\mu,\epsilon$的变量$\tau_t$服从自由度为$k-1$的t-分布。t-分布是对称钟形结构，用两边各$\alpha/2$去得到阈值。如果$\epsilon_0-\mu$在范围阈值之内，则以置信度$1-\alpha$接受假设。  
####用于算法比较的假设检验方法
- 交叉验证t-检验
	- k折交叉验证，比较算法A和B。使用$\epsilon_{Ak}-\epsilon_{Bk}$所谓新变量求均值方差和$\tau$。
	- 若新变量超出阈值范围则认为A和B有显著差异，分别求均值，小的更优。
	- k次测试集的交叉导致不是独立采样，缓解方案：$5\times 2$-交叉验证法。
- McNemar检验
	- 使用列联表(continegency table)的信息，包含这4个概率：AB都对 A对B错 A错B对 AB都错。列联表可以通过留出法很容易计算出来。
	- 假设两个算法性能相同，则A对B错 和 A错B对 的概率相同。两者差的绝对值服从正态分布，用正态分布构造卡方分布。
- Friedman检验和Nemenyi后续检验
	- 两种方法可以用于多个算法在多个测试集上的比较。是基于算法排序的检验。
	- 得到的结果是一个临界值，根据以某算法平均序值为中心的临界值域是否和别的算法临界值域相交而判断两者性能差异大小。


## 偏差方差分解
偏差方差分解(bias-variance decomposition)是解释学习算法泛化性能的一个方法。它对期望泛化错误率进行分解。  
基本的量如下：$D$-一个训练集，$x$-测试样本，$y_D(x)$-样本$x$在训练集$D$上的标签，$y(x)$-样本$x$真实对应的标签，$f(x;D)$-在训练集$D$上学得的模型对$x$的预测。
期望预测（考虑上了所有数据集）$\bar f(x)=E_D[f(x;D)]$  
使用样本数相同的不同训练集的方差（与训练集有关） $var(x)=E_D[f(x;D)-\bar f(x)]^2$  
噪声（真实标签与数据集上的标签的差平方的期望）$\epsilon^2 = E_D[y(x)-y_D(x)]^2$  
偏差（期望输出与真实标签的差平方的期望，与训练集无关）$bias^2(x)=E_D[\bar f(x)-y(x)]^2$  
期望泛化误差 $\mathcal{E}(f;D)=E_D[f(x;D)-y_D(x)]$  
假定噪声期望为0，即$E[y_D(x)-y(x)]=0$ 下面对期望泛化误差进行分解（添项与消去0项）得到$$\mathcal{E}(f;D)=var(x)+bias^2(x)+\epsilon^2$$  
期望泛化误差是方差，噪声，偏差的和。  
偏差度量了学习算法的期望预测与真实标签的差距，刻画*学习算法本身的拟合能力*。  
方差度量了同样大小的数据集变动所导致的性能的变化，刻画*数据扰动造成的影响*。  
噪声表达当前学习任务上任何学习算法所能达到的期望泛化误差的下界，刻画*问题本身的难度*。  
偏差方差分解说明泛化性能是有算法的拟合能力，数据的充分性，问题的难度共同决定。我们能控制的只有通过充分拟合数据减小偏差和提供充分的数据减小方差。  
偏差和方差之间存在冲突（bias-variance dilemma）。在训练不足时，算法拟合能力弱，数据扰动影响小，此时偏差主导泛化误差。随着训练加深，拟合能力增强，数据的扰动也被逐渐学到，方差逐渐主导泛化误差。






